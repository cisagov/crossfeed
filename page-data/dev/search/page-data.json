{"componentChunkName":"component---src-templates-documentation-page-js","path":"/dev/search/","result":{"data":{"markdownRemark":{"html":"<p>The worker is what runs scans. The code can be found in the <code class=\"language-text\">backend</code> directory.</p>\n<p>When running Crossfeed locally, the Elasticsearch cluster is run as the\n<code class=\"language-text\">crossfeed_es_1</code> Docker container. When deployed, we run an Elasticsearch cluster managed\nby <a href=\"https://aws.amazon.com/elasticsearch-service/\">Amazon Elasticsearch Service</a>.</p>\n<h3 id=\"directory-structure\" style=\"position:relative;\"><a href=\"#directory-structure\" aria-label=\"directory structure permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Directory structure</h3>\n<p>The file <code class=\"language-text\">tasks/es-client.ts</code> handles the task of interfacing with the Elasticsearch cluster.</p>\n<h3 id=\"configuration\" style=\"position:relative;\"><a href=\"#configuration\" aria-label=\"configuration permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Configuration</h3>\n<p>To configure properties for Elasticsearch, you can modify\nenvironment variables in <code class=\"language-text\">.env</code> in the root directory.</p>\n<p>If you need to configure Elasticsearch for deployment, you should update the\n<code class=\"language-text\">env.yml</code> file. You may also need to update parameters in AWS SSM, as several\nenvironment variables use values that are stored in SSM.</p>\n<!-- TODO: document environment variables -->\n<!-- Here is a list of all environment variables:\n\n| Name                            | Description                                                               | Sample value                                  |\n| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------- |\n| `REACT_APP_API_URL`             | URL for REST API                                                          | `https://api.staging.crossfeed.cyber.dhs.gov` | -->\n<h3 id=\"kibana\" style=\"position:relative;\"><a href=\"#kibana\" aria-label=\"kibana permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kibana</h3>\n<p><a href=\"https://www.elastic.co/kibana\">Kibana</a> is a tool that helps visualize and query data that is stored\nin Elasticsearch. By default, Kibana is disabled because it adds a lot of overhead to local\ndevelopment and isn't required for normally running Crossfeed locally.</p>\n<p>If you want to view a local version of Kibana (if you, for example, want to inspect the data of the\nlocal Elasticsearch instance), you should first uncomment the \"kib\" section of <code class=\"language-text\">docker-compose.yml</code>,\nre-launch Crossfeed, and then navigate to <a href=\"http://localhost:5601\">http://localhost:5601</a>.</p>\n<h3 id=\"syncing-with-the-database\" style=\"position:relative;\"><a href=\"#syncing-with-the-database\" aria-label=\"syncing with the database permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Syncing with the database</h3>\n<p>All data is populated to the database by other scans, and synchronization between the database and Elasticsearch is done by the <code class=\"language-text\">searchSync</code> scan.</p>\n<p>The <code class=\"language-text\">searchSync</code> scan retrieves all domains / services / vulnerabilities / webpages that need to be synced to Elasticsearch, then bulk\nuploads them to Elasticsearch. Afterwards, it sets the <code class=\"language-text\">syncedAt</code> column on these entities so that they will not be synced again in the future,\nuntil they are updated by other scans.</p>\n<h3 id=\"indexes-and-mapping\" style=\"position:relative;\"><a href=\"#indexes-and-mapping\" aria-label=\"indexes and mapping permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Indexes and mapping</h3>\n<p>We use a single index called \"domains\"; its name might change due to reindexing, so the current name is stored as the DOMAINS_INDEX constant in <a href=\"https://github.com/cisagov/crossfeed/blob/b55f36c0808feede82ffd8ad9473b2768e56a511/backend/src/tasks/es-client.ts#L4\">es-client.ts</a>.</p>\n<p>The domain index has a mapping. In order to create or update the mapping, you can run <code class=\"language-text\">npm run syncdb</code> from the <code class=\"language-text\">backend</code> directory. This calls\nthe <code class=\"language-text\">ESClient.syncDomainsIndex()</code>, which will update the index's mapping if it exists, or create a new index if it doesn't exist.</p>\n<p>Both <code class=\"language-text\">services</code> and <code class=\"language-text\">vulnerabilities</code> are stored with the\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.9/nested.html\">nested field type</a>. This means that they are all stored on the same domain\ndocument, and adding services / vulnerabilities will require updating / reindexing of an entire domain document.</p>\n<p>However, <code class=\"language-text\">webpages</code> are stored with the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.9/parent-join.html\">join field type</a>. This means\nthat each webpage is stored as a separate document in the \"domains\" index, but contains a value for the <code class=\"language-text\">parent_join</code> field that indicates that\nthat webpage is a child of another domain document. This makes it more efficient to add or remove single webpages, since it doesn't require\nreindexing all the webpages for a given domain.</p>\n<p>So that the webpage fields don't conflict with fields in regular parent domain records, fields in webpage records are stored with the\n<code class=\"language-text\">webpage_</code> prefix\n(<a href=\"https://github.com/cisagov/crossfeed/blob/b55f36c0808feede82ffd8ad9473b2768e56a511/backend/src/tasks/es-client.ts#L11\">see schema here</a>).</p>\n<h3 id=\"building-search-queries\" style=\"position:relative;\"><a href=\"#building-search-queries\" aria-label=\"building search queries permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Building search queries</h3>\n<p>The search query is built by the <a href=\"https://github.com/cisagov/crossfeed/blob/33fcaf4cb730974bf3d5ee61b80d13a2c675bd80/frontend/src/pages/Search/SearchProvider/buildRequest.js#L56\">buildRequest</a> function on the frontend. As of now, the logic there roughly corresponds to:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">(\n  (\n    (has a domain matching query) OR\n    (has a webpage with body matching query)\n  )\n  AND (matches filters)\n)</code></pre></div>\n<p>Search results are individual domains, but they may contain snippets of webpage bodies if they contain the webpage content. For example:</p>\n<p>![search result](./img/search result.png)</p>\n<h3 id=\"webpage-scraping\" style=\"position:relative;\"><a href=\"#webpage-scraping\" aria-label=\"webpage scraping permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Webpage scraping</h3>\n<p>Webpage scraping is done by the <code class=\"language-text\">webscraper</code> scan. This scan uses the <code class=\"language-text\">scrapy</code> Python library to follow and scrape all links, observing\nrate limits and respecting robots.txt as well.</p>\n<p>When a webpage is scraped, basic information such as the URL and status code are stored in the database through the <code class=\"language-text\">Webpage</code> model. However,\nwebpage contents and headers are not stored in the database; instead, they are directly uploaded to Elasticsearch.</p>","frontmatter":{"title":"Search","sidenav":"dev"},"fields":{"slug":"/dev/search/"},"headings":[{"value":"Directory structure","depth":3},{"value":"Configuration","depth":3},{"value":"Kibana","depth":3},{"value":"Syncing with the database","depth":3},{"value":"Indexes and mapping","depth":3},{"value":"Building search queries","depth":3},{"value":"Webpage scraping","depth":3}]}},"pageContext":{"name":"/dev/search/"}},"staticQueryHashes":["1824138477","3841949133","63159454"],"slicesMap":{}}